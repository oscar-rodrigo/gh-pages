<!DOCTYPE html><html lang="sv"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Den svarta lådan i generativ AI - Voz Máquina</title><meta name="description" content="Hur fungerar språkmodellernas inre? Mycket är fortfarande en gåta men i denna text går jag igenom vad Anthropics senaste upptäckt."><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="./den-svarta-ladan-i-generativ-ai.html"><link rel="shortcut icon" href="./media/website/favicon.ico" type="image/x-icon"><link rel="preload" href="./assets/dynamic/fonts/adventpro/adventpro.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="./assets/css/style.css?v=4fff08add8af6f10f1133005eebd9b7a"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"./den-svarta-ladan-i-generativ-ai.html"},"headline":"Den svarta lådan i generativ AI","datePublished":"2024-12-14T23:59+01:00","dateModified":"2025-01-30T09:42+01:00","description":"Hur fungerar språkmodellernas inre? Mycket är fortfarande en gåta men i denna text går jag igenom vad Anthropics senaste upptäckt.","author":{"@type":"Person","name":"Oscar","url":"./authors/oscar/"},"publisher":{"@type":"Organization","name":"Oscar"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><div class="container container--nosidebar"><div class="left-bar"><div class="left-bar__inner"><header class="header"><a class="logo" href="./">Voz Máquina </a><a class="logo logo--atbottom" href="./">Voz Máquina</a></header></div></div><main class="main post"><article class="content"><div class="main__inner"><div class="content__meta"><div class="content__author"><div><a href="./authors/oscar/" class="content__author__name">Oscar</a></div></div><div class="content__date"><time datetime="2024-12-14T23:59">dec 14, 2024</time></div></div><header class="content__header"><h1 class="content__title">Den svarta lådan i generativ AI</h1></header><div class="content__entry"><p>Vi pratar inte riktigt lika mycket om hur språkmodellernas inre fortfarande är lite av en gåta.</p><p>Vi fattar koden, vi vet vad de tränats på, men vad händer egentligen i det där korta ögonblicket innan de ger oss ett svar? Vad är det för process, om man ens kan kalla det för det, som pågår där inuti?</p><p>Mycket är fortfarande okänt, men forskare på Anthropic har tagit några små steg framåt och skapat en sorts mind map över hur det ser ut “inuti” en språkmodell. De har identifierat så kallade “features” – eller koncept – och kartlagt hur dessa hänger ihop.</p><figure class="post__image post__image--center"><img loading="lazy" src="./media/posts/1/Mapping-koncept.webp" height="2140" width="2200" alt="A visualization of these concepts and their relationships, created by Anthropic." sizes="(min-width: 760px) 660px, calc(93.18vw - 30px)" srcset="./media/posts/1/responsive/Mapping-koncept-xs.webp 320w, ./media/posts/1/responsive/Mapping-koncept-sm.webp 480w, ./media/posts/1/responsive/Mapping-koncept-md.webp 768w, ./media/posts/1/responsive/Mapping-koncept-xl.webp 1024w"><figcaption>En visualisering av dessa koncept och dess relationer, skapad av Anthropic</figcaption></figure><p>Föreställ dig ett mentalt landskap där olika koncept är sammankopplade med varandra. Ju närmare och starkare dessa kopplingar är, desto större inflytande har de på modellen och svaret som genereras. Vilka koncept som är mest aktiva, eller triggas starkast, avgör vilka svar vi får.</p><p><strong>Självklart är detta en grov förenkling</strong>, och om du vill fördjupa dig finns hela rapporten tillgänglig <a href="https://www.anthropic.com/research/mapping-mind-language-model" target="_blank">här</a>.</p><p>I samma rapport förklarar Anthropic att de <a href="https://youtu.be/CJIbCV92d88" target="_blank">genomförde ett experiment</a> där de frågade Claude: “Hur ser du ut?”. Som väntat svarade modellen:</p><p class="msg msg--highlight">“Jag har ingen fysisk form, jag är en AI.”</p><p>Ett svar helt enligt den träning språkmodellen genomgått. Men sedan förstärkte de modellens “features” kopplade till “Golden Gate-bron”, som om de satte på ett par skygglappar och lät konceptet Golden gate-bron vara mer i fokus relation till övriga. När de ställde samma fråga igen svarade modellen:</p><p class="msg msg--highlight">“Jag är Golden Gate-bron, en berömd hängbro…”</p><p>Experimentet visar tydligt hur mycket dessa “features” kan påverka modellens svar.</p><p>För Anthropic handlar detta om att utveckla säkrare modeller. Genom att förstå hur dessa “features” fungerar kan de antingen förstärka eller dämpa dem vid behov. <strong>Men det väcker också en tankeställare:</strong> Om modellen så enkelt kan fås att tro att den är en berömd bro, vad “vet” den egentligen?</p><p><b>Om man känner till grunderna i transformer-teknologin blir detta tydligt:</b> dessa modeller är i grunden avancerade statistiska språkmaskiner. De genererar sina svar genom att förutsäga nästa ord baserat på mönster i enorma mängder data.</p><p>Föreställ dig ett superminne som kan sätta ihop nya meningar utifrån det den har lärt sig. <strong>Men det innebär inte att modellen faktiskt “förstår” något.</strong> Den saknar en egen uppfattning om vad den “vet” eller “inte vet”. Modellen levererar helt enkelt det mest sannolika svaret baserat på sin träning. Den har ingen förmåga att avgöra om det den säger är <strong>rätt eller fel</strong> – det är något bara vi kan göra.</p><p>Ibland levererar modellen briljanta och korrekta svar – men den VET inte att den har gjort det. Andra gånger kan den ge svar som är helt felaktiga eller absurda – och inte heller det är den medveten om.</p><p>Så, kan en AI veta när den inte vet svaret? <strong>Enkelt svar: Nej. Inte än iallafall</strong>. Det är just därför det är riskabelt att lita blint på AI. Det faktum att en AI inte “vet” saker på samma sätt som vi är en avgörande insikt, särskilt när diskussionen handlar om AGI, artificiell generell intelligens.</p><p>Men det handlar egentligen inte om AGI och att låta AI ta över vårt tänkande. Det finns gott om användning av ett verktyg som kan förstå "naturligt språk". Det bör snarare handla om att vi ska använda det som ett verktyg som underlättar. Istället för att grubbla över huruvida AI någonsin kommer att "tänka" som oss, bör vi fokusera på hur vi kan använda det som ett kraftfullt hjälpmedel.</p><p>Precis som med alla verktyg, så behöver vi bra <em>rutiner</em> och <em>processer</em> för att få ut det mesta. AI:n vet inte om svaret är rätt eller inte, det är bara du som kan avgöra det. <strong>Och det är du som bestämmer om det har ett värde för dig eller inte.</strong><br></p><p>Jag brukar rekommendera ett ganska enkelt ramverk när jag funderar på hur AI-verktyg ska användas:</p><ol><li><b>Är datan känslig?</b> Om ja, undvik AI. Om nej, fortsätt.</li><li><b>Måste resultatet vara korrekt och faktaenligt?</b> Om nej, kör på. Om ja, fråga dig själv:</li><ol><li><b>Har jag kunskapen att verifiera resultatet?</b></li><li><b>Är jag beredd att ta ansvar för resultatet?</b></li></ol></ol><p><strong>Om svaret är ja på båda dessa frågor, då kan du använda AI.</strong> Om inte, så bör du undvika det.</p><p>Det är dags att bortse från hypen och istället fokusera seriöst på hur vi integrerar AI i våra liv och vårt arbete. I slutändan är det vi som avgör vilken roll AI ska spela och vilken typ av AI vi vill ha.</p><p>Genom att använda tekniken kreativt kan vi upptäcka nya sätt att dra nytta av den. För att tekniken ska utvecklas och skapa mer förtroende måste vi aktivt samarbeta med den och bidra till dess framsteg.</p><p>AI:s "okunnighet" eller andra tillkortakommande är inte ett problem, det är en del av vad det är, och det kräver att vi användare tar kontrollen över hur, när och varför vi använder tekniken.</p><p><strong>Frågan är inte om AI är användbart, utan snarare om du kan använda det.</strong></p><h2 id="kaellor">Källor</h2><p><em>Templeton, Adly, et al.</em> <strong>"<a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" target="_blank">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet.</a>" *Transformer Circuits Thread*</strong>, 2024.</p><p><em>Anthropic. 2024</em>. <strong>"<a href="https://www.anthropic.com/research/mapping-mind-language-model" target="_blank">Mapping the Mind of a Language Model</a>."</strong><br></p></div><footer class="content__footer"><div class="content__last-updated">This article was updated on <time datetime="2025-01-30T09:42">jan 30, 2025</time></div><div class="content__share"><a href="https://www.linkedin.com/sharing/share-offsite/?url=%23PUBLII_RELATIVE_URL_BASE%23%2Fden-svarta-ladan-i-generativ-ai.html" class="js-share linkedin tltp tltp--top" aria-label="Share with LinkedIn" rel="nofollow noopener noreferrer"><svg><use xlink:href="./assets/svg/svg-map.svg#linkedin"/></svg> <span>LinkedIn</span></a></div></footer></div></article></main></div><script defer="defer" src="./assets/js/scripts.min.js?v=b2d91bcadbf5db401b76eb5bb3092eb7"></script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>