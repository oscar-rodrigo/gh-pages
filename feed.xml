<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>oscarrodrigo</title>
    <link href="https://oscar-rodrigo.github.io/gh-pages/feed.xml" rel="self" />
    <link href="https://oscar-rodrigo.github.io/gh-pages" />
    <updated>2024-12-15T09:12:39+01:00</updated>
    <author>
        <name>Oscar</name>
    </author>
    <id>https://oscar-rodrigo.github.io/gh-pages</id>

    <entry>
        <title>Den svarta lådan</title>
        <author>
            <name>Oscar</name>
        </author>
        <link href="https://oscar-rodrigo.github.io/gh-pages/den-svarta-ladan.html"/>
        <id>https://oscar-rodrigo.github.io/gh-pages/den-svarta-ladan.html</id>

        <updated>2024-12-14T23:59:03+01:00</updated>
            <summary>
                <![CDATA[
                    Det börjar bli riktigt spännande nu. Vi har alla hört snacket om&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                
  <p>
    Det börjar bli riktigt spännande nu.
  </p>

  <p>
    Vi har alla hört snacket om AI, särskilt språkmodeller, och hur de ska förändra allt. Men vi pratar inte riktigt lika mycket om hur dessa modellers inre fortfarande är lite av en gåta.
  </p>

  <p>
    Vi fattar koden, vi vet vad de tränats på, men vad händer egentligen i det där korta ögonblicket innan de ger oss ett svar? Vad är det för process, om man ens kan kalla det för det, som pågår där inuti?
  </p>

  <p>
    Mycket är fortfarande okänt, men forskare på Anthropic har tagit några små steg framåt och skapat en sorts mind map över hur det ser ut “inuti” en språkmodell. De har identifierat så kallade “features” – eller koncept – och kartlagt hur dessa hänger ihop. Föreställ dig ett mentalt landskap där olika funktioner representeras av städer, sammankopplade med broar och vägar. Ju närmare och starkare dessa kopplingar är, desto större inflytande har de på modellen och svaret som genereras. Vilken “stad” som är mest aktiv avgör vilka svar vi får. Det fungerar som ett nätverk av idéer, där den starkaste alltid vinner.
  </p>

  <p>
    Självklart är detta en grov förenkling, och om du vill fördjupa dig finns hela rapporten tillgänglig här.
  </p>

  <p>
    I samma rapport förklarar Anthropic att de genomförde ett experiment där de frågade Claude: “Hur ser du ut?”. Som väntat svarade modellen:
  </p>

  <p class="msg msg--highlight">
    “Jag har ingen fysisk form, jag är en AI.”
  </p>

  <p>
    Inget märkligt med det. Men sedan förstärkte de modellens “features” kopplade till “Golden Gate-bron”, som om de satte på ett par glasögon med den bilden. När de ställde samma fråga igen svarade modellen:
  </p>

  <p class="msg msg--highlight">
    “Jag är Golden Gate-bron, en berömd hängbro…”
  </p>

  <p>
    Det var som om någon hade tryckt på en strömbrytare. Experimentet visar tydligt hur mycket dessa “features” kan påverka modellens svar.
  </p>

  <p>
    För Anthropic handlar detta om att utveckla säkrare modeller. Genom att förstå hur dessa “features” fungerar kan de antingen förstärka eller dämpa dem vid behov. Men det väcker också en tankeställare: Om modellen så enkelt kan fås att tro att den är en berömd bro, vad “vet” den egentligen?
  </p>

  <p>
    Om man känner till grunderna i transformer-teknologin blir detta tydligt: dessa modeller är i grunden avancerade statistiska språkmaskiner. De genererar sina svar genom att förutsäga nästa ord baserat på mönster i enorma mängder data.
  </p>

  <p>
    Föreställ dig ett superminne som kan sätta ihop nya meningar utifrån det den har lärt sig. Men det innebär inte att modellen faktiskt “förstår” något. Den saknar en egen uppfattning om vad den “vet” eller “inte vet”. Modellen levererar helt enkelt det mest sannolika svaret baserat på sin träning. Den har ingen förmåga att avgöra om det den säger är rätt eller fel – det är något bara vi kan göra.
  </p>

  <p>
    Ibland levererar modellen briljanta och korrekta svar – men den VET inte att den har gjort det. Andra gånger kan den ge svar som är helt felaktiga eller absurda – och inte heller det är den medveten om.
  </p>

  <p>
    Så, kan en AI veta när den inte vet svaret? Enkelt svar: Nej, absolut inte. Det är just därför det är riskabelt att lita blint på AI. Det faktum att en AI inte “vet” saker på samma sätt som vi är en avgörande insikt, särskilt när diskussionen handlar om AGI, artificiell generell intelligens.
  </p>

  <p>
    Men det handlar egentligen inte om AGI och att låta AI ta över vårt tänkande. Det finns gott om användning av ett verktyg som kan förstå "naturligt språk". Jag själv använder det till exempel för att skriva produktbriefer (PRD), sammanfatta mötesanteckningar och omvandla röstanteckningar till texter i specifika format. Det handlar om att vi ska använda det som ett verktyg som underlättar. Istället för att grubbla över huruvida AI någonsin kommer att "tänka" som oss, bör vi fokusera på hur vi kan använda det som ett kraftfullt hjälpmedel.
  </p>

  <p>
    Precis som med alla verktyg, så behöver vi bra rutiner och processer för att få ut det mesta. AI:n vet inte om svaret är rätt eller inte, det är bara du som kan avgöra det. Och det är du som bestämmer om det har ett värde för dig eller inte.<br>Jag brukar utgå från ett ganska enkelt ramverk när jag funderar på hur AI-verktyg ska användas:
  </p>

  <ol>
    <li>Är datan känslig? Om ja, undvik AI. Om nej, fortsätt.</li><li>Måste resultatet vara korrekt och faktaenligt? Om nej, kör på. Om ja, fråga dig själv:</li><ol><li>Har jag kunskapen att verifiera resultatet?</li><li>Är jag beredd att ta ansvar för resultatet?</li></ol>
  </ol>

  <p>
    Om svaret är ja på båda dessa frågor, då kan du använda AI. Om inte, så bör du undvika det.
  </p>

  <p>
    Det är dags att bortse från hypen och istället fokusera seriöst på hur vi integrerar AI i våra liv och vårt arbete. I slutändan är det vi som avgör vilken roll AI ska spela och vilken typ av AI vi vill ha.
  </p>

  <p>
    Genom att använda tekniken kreativt kan vi upptäcka nya sätt att dra nytta av den. För att tekniken ska utvecklas och skapa mer förtroende måste vi aktivt samarbeta med den och bidra till dess framsteg.
  </p>

  <p>
    AI:s "okunnighet" eller andra tillkortakommande är inte ett problem, det är en del av vad det är, och det kräver att vi användare tar kontrollen över hur, när och varför vi använder tekniken.
  </p>

  <p>
    <strong>Frågan är inte om AI är användbart, utan snarare om du kan använda det.</strong>
  </p>

  <p>
    
  </p>
            ]]>
        </content>
    </entry>
</feed>
